{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall torch torchvision\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "# %pip install lime\n",
    "# %pip install scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.segmentation import mark_boundaries, slic\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 31\n",
    "random.seed(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.manual_seed(seed)  \n",
    "torch.cuda.manual_seed(seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)  \n",
    "        self.relu = nn.ReLU()             \n",
    "        self.fc2 = nn.Linear(64, 10)      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "def select_images(model, test_loader, class_type = \"correct\", k = 5):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    correct_classified = []\n",
    "    incorrect_classified = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images.view(-1, 28*28))  \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                if predicted[i] == labels[i]:\n",
    "                    correct_classified.append((images[i], labels[i]))\n",
    "                else:\n",
    "                    incorrect_classified.append((images[i], labels[i], predicted[i]))\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    if class_type == \"correct\":\n",
    "        selected_indices = random.sample(range(len(correct_classified)), k)\n",
    "        return  [correct_classified[i] for i in selected_indices]\n",
    "    else:\n",
    "        selected_indices = random.sample(range(len(incorrect_classified)), k)\n",
    "        return  [incorrect_classified[i] for i in selected_indices]\n",
    "\n",
    "\n",
    "\n",
    "def visualise_digit(image_label):\n",
    "    plt.imshow(image_label[0].squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {image_label[1]}')\n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def segment_image(image_label, n_segments=10, compactness=10):\n",
    "\n",
    "    image = np.array(image_label[0]).reshape(28,28)\n",
    "    segmented_image = slic(image, n_segments=n_segments, compactness=compactness, channel_axis=None)\n",
    "    \n",
    "    return segmented_image\n",
    "\n",
    "def visualize_segments(image_label, segments):\n",
    "\n",
    "    image = np.array(image_label[0]).reshape(28,28)\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(image, segments), cmap='gray')\n",
    "    plt.title(f'Segmented Image {image_label[1]} of with ({np.unique(segments).size} segments)')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_perturbations(image_label, segments, num_samples=1000):\n",
    "    n_segments = np.unique(segments).size\n",
    "    perturbations = np.random.binomial(1, 0.7, size=(num_samples, n_segments))\n",
    "    image = np.array(image_label[0]).reshape(28,28)\n",
    "    perturbed_images = []\n",
    "    count = 0\n",
    "    for perturbation in perturbations:  \n",
    "        perturbed_image = np.copy(image)\n",
    "        for segment_idx, off in enumerate(perturbation):\n",
    "            if off == 0:  \n",
    "                perturbed_image[segments == segment_idx] = 0\n",
    "        if not np.array_equal(perturbed_image, image):\n",
    "            count += 1\n",
    "\n",
    "        perturbed_images.append((perturbed_image, image_label[1]))\n",
    "    # print(count)\n",
    "    return perturbed_images\n",
    "\n",
    "\n",
    "\n",
    "def predict_digits(model, image_labels):\n",
    "\n",
    "    model.eval()  \n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for image, true_label in image_labels:\n",
    "            image_tensor = torch.tensor(image, dtype=torch.float32).view(1, 28*28)\n",
    "            output = model(image_tensor)\n",
    "            predicted_label = torch.argmax(output, dim=1).item()\n",
    "            predictions.append(predicted_label)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def fit_linear_model(image_label, perturbations, predictions, kernel_width=2):\n",
    "\n",
    "    perturbated_images = np.array([perturbation[0] for perturbation in perturbations])\n",
    "    perturbated_images = perturbated_images.reshape(perturbated_images.shape[0], -1) \n",
    "\n",
    "    original_image = np.array(image_label[0]).reshape(1, -1)\n",
    "\n",
    "\n",
    "    distances = np.sqrt(np.sum((perturbated_images - original_image) ** 2, axis=1))\n",
    "    weights = np.exp(-(distances ** 2) / kernel_width ** 2)\n",
    "    \n",
    "    linear_model = Ridge(alpha=0.1)\n",
    "    linear_model.fit(perturbated_images, predictions, sample_weight=weights)\n",
    "    \n",
    "    coefficients = linear_model.coef_\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "def visualise_coefficients(original_image, coefficients, figsize=(12, 10)):\n",
    "\n",
    "\n",
    "    orig_img = original_image.reshape(28, 28)\n",
    "    coef_img = coefficients.reshape(28, 28)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=figsize)    \n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    coef_plot = plt.imshow(coef_img, cmap='magma')\n",
    "    plt.title('Coefficient Values\\n(Blue=Positive, Red=Negative)')\n",
    "    plt.colorbar(coef_plot)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    abs_coef = np.abs(coef_img)\n",
    "    abs_plot = plt.imshow(abs_coef, cmap='viridis')\n",
    "    plt.title('Absolute Importance')\n",
    "    plt.colorbar(abs_plot)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    max_abs_coef = np.max(np.abs(coef_img))\n",
    "    norm_coef = coef_img / (max_abs_coef + 1e-10)\n",
    "    \n",
    "    rgb_img = np.zeros((28, 28, 3))\n",
    "    for i in range(3):\n",
    "        rgb_img[:, :, i] = orig_img / 2.0\n",
    "    \n",
    "    green_mask = norm_coef > 0\n",
    "    rgb_img[:, :, 1][green_mask] += norm_coef[green_mask] * 0.5\n",
    "    red_mask = norm_coef < 0\n",
    "    rgb_img[:, :, 0][red_mask] += -norm_coef[red_mask] * 0.5\n",
    "    rgb_img = np.clip(rgb_img, 0, 1)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title('Coefficient Overlay\\n(Green=Positive, Red=Negative)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(orig_img, cmap='gray')\n",
    "    contour = plt.contour(coef_img, levels=10, colors='r', alpha=0.8)\n",
    "    plt.clabel(contour, inline=True, fontsize=8)\n",
    "    plt.title('Coefficient Contours on Original Image')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DigitClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting correctly classified fimages\n",
    "correctly_classified_images = select_images(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_with_lime(selected_images = [], n_segments = 10, kernel_width = 10):\n",
    "\n",
    "    for id, image_label in enumerate(selected_images):\n",
    "        print(f'{id+1}. Selected digit')\n",
    "        visualise_digit(image_label)\n",
    "\n",
    "        segmented_image  = segment_image(image_label, n_segments=n_segments)\n",
    "        visualize_segments(image_label, segmented_image)\n",
    "\n",
    "        print(\"Generating perturbations : \")\n",
    "        perturbed_images = generate_perturbations(image_label, segmented_image)\n",
    "        \n",
    "        print(\"Perturbed Images are : \")\n",
    "        for perturbed_id, perturbed_image in enumerate(perturbed_images[:4]): #showing only 4 examples per image\n",
    "            print(f'{perturbed_id}')\n",
    "            visualise_digit(perturbed_image)\n",
    "\n",
    "        \n",
    "        #predicting labels of these images\n",
    "        predicted_digits = predict_digits(model, perturbed_images)\n",
    "\n",
    "        #fitting linear model\n",
    "        coefficients = fit_linear_model(image_label, perturbed_images, predicted_digits, kernel_width=kernel_width)\n",
    "\n",
    "        #visualising coefficients\n",
    "        visualise_coefficients(image_label[0], coefficients)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with kernel widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=10, kernel_width=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=10, kernel_width=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=10, kernel_width=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=10, kernel_width=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different segment numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=4, kernel_width=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=10, kernel_width=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_with_lime(correctly_classified_images, n_segments=30, kernel_width=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
